<h1 align="center">Prompt-Tuning</h1>

+ A pipeline for Prompt-tuning
+ 集成主流的Prompt-tuning相关方法，以及search template策略
+ 提供Prompt-tuning完整的执行pipeline

# 说明

# Paper
+ [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/pdf/2001.07676.pdf)
+ [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/pdf/2010.15980.pdf)
+ [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/pdf/2012.15723.pdf)
+ [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
+ [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf) | [阅读笔记](https://kexue.fm/archives/8295)
+ [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
+ [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://arxiv.org/pdf/2108.04106.pdf)
+ [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/pdf/2109.04332.pdf)
+ [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](https://arxiv.org/pdf/2110.07904.pdf)

# Reference
+ https://github.com/princeton-nlp/LM-BFF
+ https://github.com/shmsw25/Channel-LM-Prompting

# Dataset
+ GLUE：https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar
```shell
sh scripts/download_glue_dataset.sh
```
+ CLUE：https://github.com/CLUEbenchmark/CLUE
```shell
sh scripts/download_clue_dataset.sh
```



